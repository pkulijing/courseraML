\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\section{Introduction}
Machine learning enables computer to complete tasks without being explicitly programmed. With appropriate methods applied, computer can obtain the ability to better complete a task by learning from previous results with respect to this task, which resembles the learning process of human beings. By ``better'', we mean better performance under the evaluation of some sort of quantative measurement.

Machine learning can be divided into {\bf supervised learning} and {\bf unsupervised learning} according to the data set used. 

In supervised learning, the correct output of each case in the data set is already known. The learning algorithm is supposed to reveal the relationship between the input and the output. Supervised learning problems can be categorized into {\bf regression} problems and {\bf classification} problems. In regression problems, the output has continuous value, while in classification the output is discrete.

In unsupervised learning, no correct output is provided in advance. Structure of the data needs to be derived by {\bf clustering} the data based on the relationship among the variables in the data set.

\section{Univariate linear regression}
\subsection{Hypothesis function and cost function}
Linear regression with one variable, i.e. univariate linear regression, is the simplest regression problem. A single output needs to be predicted from a single input based on a given dataset containing a series of input/output pairs in which the outputs are correct.

Since it is assumed that the input and the output are linearly relative, we take the {\bf hypothesis function}
\begin{equation}
h_{\theta}(x) = \theta_0 + \theta_1x
\end{equation}

Our target is to find the approriate parameters $\theta_0, \theta_1$ that minimize the {\bf cost function}
\begin{equation}
J(\theta_0, \theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)}-y^{(i)}\right)^2
\end{equation}
in which $\left(x^{(i)},y^{(i)}\right), i = 1, 2 \dots m$ are the training examples, and $m$ is the size of the training set. In mathematical languague, the problem we are supposed to solve is $\min\limits_{\theta_0, \theta_1}J\left(\theta_0, \theta_1\right)$.
\subsection{Gradient descent}
A common method to help automatically find the optimal parameters $\theta_0, \theta_1$ is {\bf gradient descent}. It starts with some random $\theta_0, \theta_1$, and iteratively alter the values of $\theta_0, \theta_1$ according to the rules
\begin{equation}\label{gddef}
\theta_j \coloneqq \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)
\end{equation}
until convergence.

Gradient descent algorithm does not necessarily converge to the global minimum for any function. If there exists a few local minimums, the algorithm could wind up at any of them with different initial choices of $\theta_j$. However, cost functions admit only one local minimum, which is hence also the global minimum. Thus gradient descent can be applied here without having to worry about the possibility of converging to a local minimum due to unwise choice of initial value.

For univariate linear regression, \eqref{gddef} becomes
\begin{equation}
\begin{split}
\theta_0 &\coloneqq \theta_0 - \frac{\alpha}{m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)\\
\theta_1 &\coloneqq \theta_1 - \frac{\alpha}{m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)x^{(i)}
\end{split}
\end{equation}
\ifx\PREAMBLE\undefined
\end{document}
\fi