\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\section{Multivariate linear regression}
\subsection{Hypothesis function and cost function}
For linear regression problem with multiple variables rather than one single variable, we have the hypothesis function
\begin{equation}
h_{\theta}(x) = \theta^{\mathsf T}x = \sum\limits_{i=0}^{n}\theta_ix_i 
\end{equation}
in which $x_0 \equiv 1$ and $n$ is the number of features(variables) being considered. And the cost function is 
\begin{equation}
J(\theta) = J(\theta_0, \theta_1, \dots , \theta_n) = \frac{1}{2m}\sum\limits_{i=1}^{m}\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2
\end{equation}

And the gradient descent algorithm becomes
\begin{equation}
\theta_j \coloneqq \theta_j - \frac{\alpha}{m}\sum\limits_{i=1}^{m}\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)x^{(i)}_j
\end{equation}
in which $j = 0, 1 \dots n$.
\subsection{Feature scaling}
Making sure that values of all the features are on similar scale helps gradient descent to converge much more quickly. A practical choice would be to limit each feature into the range of (-1,1). Note that this does not go for $x_0\equiv1$.

If a feature $x_i$ is guaranteed to range from $a$ to $b$, we can use {\bf mean normalization} and substitute $x_i$ with $\frac{x_i-(a+b)/2}{b-a}$. It is also fine to use $\frac{x_i-\mu_i}{\sigma_i}$, in which $\mu_i$ and $\sigma_i$ are respectively the average and standard deviation of $x_i$.
\subsection{Learning rate \texorpdfstring{$\alpha$}{}}
For small enough learning rate $\alpha$, the cost function should decrease on every iteration. But if $\alpha$ is too small, the convergence may become extremely slow. And if $\alpha$ is too large, the cost function might diverge with the iteration. Name $k$ the number of iterations, drawing the $J(\theta)-k$ plot could help to choose an appropriate learning rate.
\subsection{Analytical solution: normal equation}
Minimum of the cost function can be found analytically by solving the normal equation 
\begin{equation}
\frac{\partial}{\partial \theta_j}J(\theta) = 0, j = 0, 1, \dots, n
\end{equation}
For the multivariate linear regression problem, if we define the matrix $X$
\begin{equation}
X=\begin{bmatrix}
\left(x^{(1)}\right)^{\mathsf T}\\
\left(x^{(2)}\right)^{\mathsf T}\\
\dots\\
\left(x^{(m)}\right)^{\mathsf T}
\end{bmatrix}
=\begin{bmatrix}
1 & x^{(1)}_1 & x^{(1)}_2 & \dots &  x^{(1)}_n\\
1 & x^{(2)}_1 & x^{(2)}_2 & \dots &  x^{(2)}_n\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1 & x^{(m)}_1 & x^{(m)}_2 & \dots &  x^{(m)}_n\\
\end{bmatrix}
\end{equation}
and the vector 
\begin{equation}
y=
\begin{bmatrix}
y^{(1)}\\
y^{(2)}\\
\dots\\
y^{(m)}
\end{bmatrix}
\end{equation}
it can be verified that the optimal $\theta$ would be 
\begin{equation}
\theta = \left(X^{\mathsf T}X\right)^{-1}X^{\mathsf T}y
\end{equation}

Normal equation method does not require a learning rate to be chosen. Neither is iteration needed. However, since it requires the calculation of $\left(X^{\mathsf T}X\right)^{-1}$, it can become quite slow when $n$ is large, whereas gradient descent still works fine.

What's more, it is possible that $X^{\mathsf T}X$ is non-invertible (singular, degenerate) when there are redundant features or when there are too many features ($n>m$). In such cases, the $pinv$ (persudo inverse) function in octave and matlab will help avoid failure of the calculation. We could also manually delete redundant features or consider using regularisation, which will be covered later.
\ifx\PREAMBLE\undefined
\end{document}
\fi